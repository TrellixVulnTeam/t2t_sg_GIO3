* Transformer Small

** Data Generation

*** Problems base classes

- blog: https://cloud.google.com/blog/big-data/2018/02/cloud-poetry-training-and-hyperparameter-tuning-custom-text-models-on-cloud-ml-engine

1. Models that use a sequence as an input, but are essentially
   classifiers or regressors—a spam filter or sentiment
   identifier are canonical examples of such a model.
2. Models that take a single entity as input but produce a text
   sequence as output—image captioning is an example of such a
   model, since given an image, the model needs to produce a
   sequence of words that describes the image.
3. Models that take sequences as input and produce sequences as
   outputs. Language translation, question-answering, and text
   summarization are all examples of this third type.
 
 
*** Registry Gotcha

- github: https://github.com/tensorflow/tensor2tensor/blob/master/docs/new_problem.md
- blog: https://cloud.google.com/blog/big-data/2018/02/cloud-poetry-training-and-hyperparameter-tuning-custom-text-models-on-cloud-ml-engine
- tutorial: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/09_sequence/poetry.ipynb

You can test data generation of your a problem in your own
project with:

#+BEGIN_SRC bash
PROBLEM=poetry_line_problem
DATA_DIR=$HOME/t2t_data
TMP_DIR=/tmp/t2t_datagen
mkdir -p $DATA_DIR $TMP_DIR

t2t-datagen \
  --t2t_usr_dir=$PATH_TO_YOUR_PROBLEM_DIR \
  --data_dir=$DATA_DIR \
  --tmp_dir=$TMP_DIR \
  --problem=$PROBLEM
#+END_SRC

#+BEGIN_SRC python
@registry.register_problem
class PoetryLineProblem(text_problems.Text2TextProblem):
    pass
#+END_SRC

Where:
- The Python Filename is ~snake_case~ of problem name
- ~PROBLEM~ is the name of the class that was registered with
  ~@registry.register_problem()~, but converted from ~CamelCase~ to
  ~snake_case~



** Input Pipeline

*** batch generation

https://github.com/tensorflow/tensor2tensor/blob/master/docs/overview.md#batching

Variable length Problems are bucketed by sequence length and then
batched out of those buckets. This significantly improves
performance over a naive batching scheme for variable length
sequences because each example in a batch must be padded to match
the example with the maximum length in the batch.

**** Implementation

- In ~data_generators/problems.py~ ~Problem.input_fn()~

#+BEGIN_SRC python
# sg: GPU batch size / buckets are generated here
dataset = data_reader.bucket_by_sequence_length(
    dataset, data_reader.example_length, batching_scheme["boundaries"],
    batching_scheme["batch_sizes"])
# bucket_by_sequence_length using tf.contrib.data.group_by_window()
#+END_SRC

https://www.tensorflow.org/api_docs/python/tf/contrib/data/group_by_window

A transformation that groups windows of elements by key and
reduces them.

This transformation maps each consecutive element in a dataset to
a key using key_func and groups the elements by key. It then
applies reduce_func to at most window_size_func(key) elements
matching the same key. All execpt the final window for each key
will contain window_size_func(key) elements; the final window may
be smaller.


** T2TModel

~top()~ ~body()~ ~bottom()~ ~loss()~

From https://github.com/tensorflow/tensor2tensor/blob/master/docs/overview.md#building-the-model

#+BEGIN_QUOTE
At this point, the input features typically have ~"inputs"~ and ~"targets"~,
each of which is a batched 4-D Tensor (e.g. of shape ~[batch_size,
sequence_length, 1, 1]~ for text input or ~[batch_size, height, width, 3]~ for
image input).
#+END_QUOTE

#+BEGIN_QUOTE
The Estimator model function is created by ~T2TModel.estimator_model_fn~, which
may be overridden in its entirety by subclasses if desired. Typically,
subclasses only override ~T2TModel.body~.
#+END_QUOTE

- ~estimator_model_fn~ is a ~@classmethod~ function, which is
  used as an override of the original constructor. This acts like
  a factory function return subclass instances of ~T2TModel~ class

#+BEGIN_QUOTE
The model function constructs a ~T2TModel~, calls it, and then calls
~T2TModel.{estimator_spec_train, estimator_spec_eval, estimator_spec_predict}~
depending on the mode.

A call of a ~T2TModel~ internally calls ~bottom~, ~body~, ~top~, and ~loss~, all
of which can be overridden by subclasses (typically only ~body~ is).

The default implementations of ~bottom~, ~top~, and ~loss~ depend on the
~Modality~ specified for the input and target features (e.g.
~SymbolModality.bottom~ embeds integer tokens and ~SymbolModality.loss~ is
~softmax_cross_entropy~).
#+END_QUOTE


** Function Flow



~bottom~, ~top~, and ~loss~ are specified in ~hparams.problems~

1. features flow into model: t2t_model.py
   ~T2TModel.estimator_model_fn()~ calls ~logits, losses_dict =
   model(features)~ 
2. ~model(features)~ calls ~Layers.base.Layer.__call__()~ which
   is overridden by ~T2TModel.call()~ 
3. ~T2TModel.call()~ calls ~model_fn_sharded(sharded_features)~
4. ~model_fn_sharded()~ calls
   ~model_fn(datashard_to_features)~
5. ~model_fn()~ calls ~bottom()~, ~body()~ ~top()~, and ~loss()~
   in order
   - ~body()~ can return ~losses~ or not 
     - If return, then it must contains ~(logits, losses)~ in a
       ~tuple~ then training will skip ~top()~ and ~loss()~
       functions. This means ~body()~ need to implement ~top()~
       and ~loss()~ by itself
     - Otherwise, it simply return ~logits~ as output. Then
       ~model_fn()~ will use ~top()~ to calculate ~logits~ and
       ~loss()~ to calculate ~loss()~







loss

** Decoder Notes

- ~has_input=False~, no encoder


*** Pre & Post Process

For example, if sequence=="dna", then the output is
~previous_value + normalize(dropout(x))~

#+BEGIN_SRC python
hparams.layer_preprocess_sequence = "n"
# normalize(x)
hparams.layer_postprocess_sequence = "da"
# previous_value + dropout(x)
#+END_SRC


** Encoder-Decoder Diffs

*** self-attention

No computational differences

- layer name is different (same name with format string /
  variable)
- bias is different
- decoder has layer cache but currently un-implemented
